{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN4U569cOpfNZkTDg0p6JC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srrmtt/AML/blob/main/laboratory_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P0inYd5_exiR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir(\"./AML-Homework3-DA\"):\n",
        "  !git clone \"https://github.com/antoalli/AML-Homework3-DA.git\" \n",
        "  !mv \"./AML-Homework3-DA/alexnet.py\" \"./alexnet.py\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from alexnet import *\n",
        "class MyImageDataset(Dataset):\n",
        "  def __init__(self, img_file:str, transform=None):\n",
        "    self.samples = list()\n",
        "    self.transform = None\n",
        "    with open(img_file, 'r') as f_in:\n",
        "      for line in f_in:\n",
        "        path, label = line.split(' ')\n",
        "        path = f'./AML-Homework3-DA/PACS/{path}'\n",
        "        image = read_image(path)\n",
        "        self.samples.append((image, int(label)))\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    image, label = self.samples[idx]\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image, label\n"
      ],
      "metadata": {
        "id": "FH45F33rvQfW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "aioVVt3Cy_zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cartoon_dataset = MyImageDataset('./AML-Homework3-DA/txt_lists/cartoon.txt')\n",
        "photo_dataset = MyImageDataset('./AML-Homework3-DA/txt_lists/photo.txt')\n",
        "cartoon_len = len(cartoon_dataset)\n",
        "photo_len = len(photo_dataset)\n",
        "\n",
        "print(f\"cartoon: {cartoon_len} photo: {photo_len}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kfwqKvvy_AW",
        "outputId": "d93f6711-1f41-461b-be50-00446a1a1495"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cartoon: 2344 photo: 1670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 32\n",
        "train_cartoon = DataLoader(cartoon_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "train_photo = DataLoader(photo_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "alex_net = AlexNet(7)\n",
        "nn = AlexNetDA(alex_net, num_domains=2)\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #'cpu' # 'cuda' or 'cpu'\n",
        "\n",
        "nn.to(DEVICE)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dsTQWFbOfYwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Loop"
      ],
      "metadata": {
        "id": "AmaSS1422svh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import Tensor\n",
        "from tqdm import tqdm\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "\n",
        "source_gt = torch.zeros(batch_size).to(DEVICE)\n",
        "target_gt = torch.ones(batch_size).to(DEVICE)\n",
        "\n",
        "label_predictor_loss = []\n",
        "domain_class_source_loss = []\n",
        "domain_class_target_loss = []\n",
        "ce = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in tqdm(range(NUM_EPOCHS)):\n",
        "  for i, batch in enumerate(train_photo):\n",
        "    # optimizer.zero_grad\n",
        "    images, labels = batch\n",
        "    class_pred = nn(images)\n",
        "    loss_cls = ce(class_pred, labels)\n",
        "\n",
        "    # loss_tot.backward()\n",
        "    # optimizer.step\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IYc6HogN2vKU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}